{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.43.1\n",
      "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 43.7/43.7 kB 711.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (2024.5.15)\n",
      "Requirement already satisfied: requests in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from transformers==4.43.1) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.1) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from tqdm>=4.27->transformers==4.43.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from requests->transformers==4.43.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from requests->transformers==4.43.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from requests->transformers==4.43.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\studies\\hdsassignments\\pmim702\\.conda\\lib\\site-packages (from requests->transformers==4.43.1) (2024.7.4)\n",
      "Downloading transformers-4.43.1-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB 6.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.4/9.4 MB 10.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.4 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.9/9.4 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.5/9.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/9.4 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.5/9.4 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.1/9.4 MB 10.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.6/9.4 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.1/9.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.6/9.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.4 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.4 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.4 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "Successfully installed transformers-4.43.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.43.1\n",
    "# 4.42.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets\n",
    "# !pip install -q bitsandbytes\n",
    "# !pip install -q peft\n",
    "# !pip install -q accelerate\n",
    "# !pip install -q trl\n",
    "# !pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility native libraries\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "# installed libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# quantization libraries\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884415a0d25b47aba05d943669ac43fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from custom_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility native libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# installed libraries\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# quantization libraries\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class DownloadModel:\n",
    "\n",
    "    def _quantize_model(self, model_artifact: dict) -> dict:\n",
    "        \"\"\"\n",
    "        A function to quantize the model using PEFT and LoRA\n",
    "        \"\"\"\n",
    "        model = model_artifact[\"model\"]\n",
    "        config = LoraConfig(\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "                \"lm_head\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            lora_dropout=0.05,\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, config)\n",
    "        model_artifact[\"model\"] = model\n",
    "\n",
    "        return model_artifact\n",
    "\n",
    "\n",
    "    def _get_mistral_model(self):\n",
    "        \"\"\"\n",
    "        A function to download the mistral model from\n",
    "        \"\"\"\n",
    "        model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, quantization_config=bnb_config)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        model_artifact = self._quantize_model(model_artifact)\n",
    "        return model_artifact\n",
    "\n",
    "    def _get_llama_model(self):\n",
    "        \"\"\"\n",
    "        A function to download the llama model from\n",
    "        \"\"\"\n",
    "        model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        # TODO: fix  the quantization logic for llama support\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id, quantization_config=bnb_config)\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        model_artifact = self._quantize_model(model_artifact)\n",
    "        return model_artifact\n",
    "\n",
    "\n",
    "    def _get_eluether_model(self):\n",
    "        \"\"\"\n",
    "        A function to download the eluether model from\n",
    "        \"\"\"\n",
    "        model_id = \"EleutherAI/pythia-70m\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        return model_artifact\n",
    "\n",
    "\n",
    "    def _get_hermes_model(self):\n",
    "        \"\"\"\n",
    "        A function to download the eluether model from\n",
    "        \"\"\"\n",
    "        model_id = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id, quantization_config=bnb_config)\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        model_artifact = self._quantize_model(model_artifact)\n",
    "        return model_artifact\n",
    "    \n",
    "    def download(self, model_name):\n",
    "        \"\"\"\n",
    "        A function to interface the different model downloaders\n",
    "\n",
    "        Args:\n",
    "            model_name: str: name of the model to be downloaded\n",
    "        \n",
    "        Returns:\n",
    "            the response of the model download function\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        model_map = {\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.2\": self._get_mistral_model,\n",
    "            \"NousResearch/Hermes-3-Llama-3.1-8B\": self._get_hermes_model,\n",
    "            \"EleutherAI/pythia-70m\": self._get_eluether_model,\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\": self._get_llama_model\n",
    "        }\n",
    "        if model_name not in model_map:\n",
    "            raise ValueError(\"Model not found\")\n",
    "        \n",
    "        return model_map[model_name]()\n",
    "\n",
    "class PromptTemplates:\n",
    "    def _input_output_template(self):\n",
    "        pass\n",
    "\n",
    "    def _icl_template(self):\n",
    "        \"\"\"\n",
    "        A function to create the instruct response template\n",
    "\n",
    "        Returns:\n",
    "        dict: with response: \n",
    "            : without response:\n",
    "\n",
    "        Raises:\n",
    "        None\n",
    "        \"\"\"\n",
    "        text_template_with_response = \"\"\"\n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "\n",
    "            ### Instruction:\n",
    "            1. Understand the clinical text\n",
    "            2. Extract the medicine prescription names\n",
    "            3. Extract the dosage of medicine prescriptions\n",
    "            4. Extract the seizure frequency\n",
    "            5. The parts of sentences in <emphasize> tags are important\n",
    "            5. structure  the response in the json format as mentioned in the example\n",
    "            response\n",
    "\n",
    "            ### Input:\n",
    "            {input}\n",
    "\n",
    "            ### Response:\n",
    "            {response}\n",
    "        \"\"\"\n",
    "\n",
    "        text_template_without_response = \"\"\"\n",
    "            Given the instruction, please write the response in the json format\n",
    "\n",
    "            ### Instruction:\n",
    "            Given the clinical text, please\n",
    "            1. Extract the medicine prescription names\n",
    "            2. Extract the dosage of medicine prescriptions\n",
    "            3. Extract the seizure frequency\n",
    "            4. structure the response in the json format\n",
    "\n",
    "\n",
    "            ### Clinical Text:\n",
    "            {clinical_text}\n",
    "\n",
    "            ### Response:\n",
    "        \"\"\"\n",
    "        with_response = []\n",
    "        without_response = []\n",
    "\n",
    "        questions = glob.glob(\"/content/modified_data/*.txt\")\n",
    "        responses = glob.glob(\"/content/modified_data/*.json\")\n",
    "\n",
    "        for clinical_text_file, response_file in zip(questions, responses):\n",
    "            with open(clinical_text_file, \"r\") as f:\n",
    "                clinical_text = f.read()\n",
    "\n",
    "            with open(response_file, \"r\") as f:\n",
    "                response = json.load(f)\n",
    "\n",
    "            text_with_prompt_template_qa = text_template_with_response.format(\n",
    "                clinical_text=clinical_text, response=response)\n",
    "            with_response.append({\n",
    "                \"text\": text_with_prompt_template_qa})\n",
    "\n",
    "            text_with_prompt_template_q = text_template_without_response.format(\n",
    "                clinical_text=clinical_text)\n",
    "            without_response.append(\n",
    "                {\"test_input\": text_with_prompt_template_q,\n",
    "                \"response\": response})\n",
    "\n",
    "        return {\n",
    "            \"with_response\": with_response,\n",
    "            \"without_response\": without_response\n",
    "        }\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "\n",
    "    def _input_output_template_preprocess(self, tokenizer, data_path, max_length=250):\n",
    "        \"\"\"\n",
    "        A function to create the input output template preprocess\n",
    "\n",
    "        Args:\n",
    "            tokenizer: object: tokenizer object to encode / decode \n",
    "            data_path: str: path of the data\n",
    "        \n",
    "        Returns:\n",
    "            data: pd.dataframe: data frame containing the templates and encoded data\n",
    "\n",
    "        Raises:\n",
    "            none \n",
    "        \n",
    "        \"\"\"\n",
    "        dataset = {\"input\": [], \"output\": [], \"text\": []}\n",
    "        \n",
    "        file_path = f\"{data_path}/tmp.jsonl\"\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "        for txt_file, json_file in zip(\n",
    "            glob.glob(f\"{data_path}/*.txt\"), glob.glob(f\"{data_path}/*.json\")):\n",
    "            \n",
    "            # read the txt files\n",
    "            with open(txt_file, \"r\") as f:\n",
    "                text_data = f.read()\n",
    "\n",
    "            # read the json files\n",
    "            with open(json_file, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                json_data = str(json_data)\n",
    "            \n",
    "            text_format = \"\"\" \n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction:\n",
    "                1. Understand the clinical text.\n",
    "                2. Extract the medicine prescription names.\n",
    "                3. Extract the dosage of medicine prescriptions.\n",
    "                4. Extract the seizure frequency.\n",
    "                5. The parts of sentences in <emphasize> tags are important.\n",
    "                5. structure  the response in the json format as mentioned in ### Response section. \\n ### Input:  {input} \\n ### Response: {output} \\n\n",
    "            \"\"\"\n",
    "            test_format = \"\"\" \n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction:\n",
    "                1. Understand the clinical text.\n",
    "                2. Extract the medicine prescription names.\n",
    "                3. Extract the dosage of medicine prescriptions.\n",
    "                4. Extract the seizure frequency.\n",
    "                5. The parts of sentences in <emphasize> tags are important.\n",
    "                5. structure  the response in the json format as mentioned in ### Response section. \\n ### Input:  {input} \\n ### Response:\\n\n",
    "            \"\"\"\n",
    "            data_line = {\n",
    "                \"input\": text_data,\n",
    "                \"output\": json_data,\n",
    "                \"text\": text_format.format(input=text_data, output=json_data),\n",
    "                \"test_prompt\": test_format.format(input=text_data)\n",
    "            }\n",
    "\n",
    "            with open(file_path, \"a\") as fw:\n",
    "                json.dump(data_line, fw)\n",
    "        \n",
    "        def _tokenize_data(example, max_length):\n",
    "            text = example[\"text\"][0]\n",
    "            tokenized_inputs = tokenizer(\n",
    "                text,\n",
    "                padding=True,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "\n",
    "            max_length = min(\n",
    "                len(tokenized_inputs[\"input_ids\"]),\n",
    "                max_length\n",
    "            )\n",
    "\n",
    "            tokenized_inputs = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "\n",
    "            return tokenized_inputs\n",
    "            \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        # assumption that the important values are found on the left of the data \n",
    "        tokenizer.truncation_side = \"right\"\n",
    "        \n",
    "        data = datasets.load_dataset(\"json\", data_files=f\"{data_path}/tmp.jsonl\", split=\"train\")\n",
    "        partial_tokenize_data = functools.partial(_tokenize_data, max_length=max_length)\n",
    "        tokenized_dataset = data.map(partial_tokenize_data)\n",
    "        tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "        return tokenized_dataset\n",
    "\n",
    "        \n",
    "    def preprocess(self, tokenizer, data_path):\n",
    "        \"\"\"\n",
    "        A factory method function to interface the template associated preprocess function\n",
    "\n",
    "        Args:\n",
    "            # template_type: str: type of the template\n",
    "            tokenizer: object: tokenizer object to decode / encode the text data\n",
    "            data_path: str: path of the dataset repository\n",
    "        \n",
    "        Returns:\n",
    "            data: pd.Dataframe: data frame containing the input-output-inputids-text-attention_mask\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # template_interfaces = {\n",
    "        #     \"icl\": self._icl_template_preprocess,\n",
    "        #     \"instruct\": self._input_output_template_preprocess\n",
    "        # }\n",
    "        # return template_interfaces[template_type](**kwargs)\n",
    "        return self._input_output_template_preprocess(tokenizer=tokenizer, data_path=data_path)\n",
    "\n",
    "\n",
    "class Postprocess:\n",
    "    def _postprocess(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def _fine_tune_model(self, model_name, model, tokenizer, tokenized_dataset, k_split=1, max_length=250):\n",
    "        \"\"\"\n",
    "        A function to train the model using the fine tuning technique\n",
    "\n",
    "        Args:\n",
    "            model_name: str: name of the model\n",
    "            model: object: model object \n",
    "            data: pd.DataFrame: pandas dataframe containing the data\n",
    "            k_split: int: number of splits to be made in the dataset\n",
    "        \n",
    "        Returns:\n",
    "            finetuned_model: dict: model fine tuned on the data and the tokenizer\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        # wandb setup\n",
    "        project = \"demo\"\n",
    "        run_name = project + model_name.split(\"/\")[0].upper()\n",
    "        output_dir = \"./\" + run_name\n",
    "\n",
    "\n",
    "        # kfold_splitter = KFold(n_splits=1, shuffle=True, random_state=42)\n",
    "\n",
    "        # training arguments\n",
    "        train_args = transformers.TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                warmup_steps=5,\n",
    "                per_device_train_batch_size=1,\n",
    "                gradient_checkpointing=True,\n",
    "                gradient_accumulation_steps=4,\n",
    "                max_steps=100,\n",
    "                learning_rate=1.0e-4, # Want about 10x smaller than the Mistral learning rate\n",
    "                logging_steps=50,\n",
    "                bf16=False,\n",
    "                optim=\"paged_adamw_8bit\",\n",
    "                logging_dir=\"./logs\",        # Directory for storing logs\n",
    "                save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "                save_steps=50,                # Save checkpoints every 50 steps\n",
    "                eval_strategy=\"steps\", # Evaluate the model every logging step\n",
    "                eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "                do_eval=True,                # Perform evaluation at the end of training\n",
    "                report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "                run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "            )\n",
    "        \n",
    "        # memory footprint\n",
    "        model_flops = (\n",
    "            model.floating_point_ops(\n",
    "                {\n",
    "                \"input_ids\": torch.zeros(\n",
    "                    (1, max_length)\n",
    "                )\n",
    "                }\n",
    "            )\n",
    "            * train_args.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        print(model)\n",
    "        print(\"Memory footprint\", model.get_memory_footprint() / 1e9, \"GB\")\n",
    "        print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "        \n",
    "        # k fold cross validation\n",
    "        # num_split = tqdm(total=k_split)\n",
    "        # for train_index, test_index in kfold_splitter.split(tokenized_dataset[\"text\"]):\n",
    "        #     num_split.update(1)\n",
    "        #     train_dataset = [tokenized_dataset[\"text\"][i] for i in train_index]\n",
    "        #     eval_dataset = [tokenized_dataset[\"text\"][i] for i in test_index]\n",
    "            \n",
    "        #     if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "        #         model.is_parallelizable = True\n",
    "        #         model.model_parallel = True\n",
    "            \n",
    "        #     trainer = transformers.Trainer(\n",
    "        #         model=model,\n",
    "        #         train_dataset=train_dataset,\n",
    "        #         eval_dataset=eval_dataset,\n",
    "        #         args=train_args,\n",
    "        #         data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        #             tokenizer, mlm=False),\n",
    "        #     )\n",
    "\n",
    "        #     model.config.use_cache = False  \n",
    "            \n",
    "        #     trainer.train()\n",
    "        # num_split.close()\n",
    "        split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "        train_dataset = split_dataset[\"train\"]\n",
    "        test_dataset = split_dataset[\"test\"]\n",
    "        trainer = transformers.Trainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            args=train_args,\n",
    "            data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "                    tokenizer, mlm=False)\n",
    "        )\n",
    "        model.config.use_cache = False\n",
    "        trainer.train()\n",
    "        model.save_pretrained(output_dir)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _icl_model(self, model_name, model, tokenizer, tokenized_dataset, k_split=1):\n",
    "        \"\"\"\n",
    "        A function to train the model using the in-context learning technique\n",
    "\n",
    "        Args:\n",
    "            model_name: str: name of the model\n",
    "            model: object: model object \n",
    "            data: pd.DataFrame: pandas dataframe containing the data\n",
    "            k_split: int: number of splits to be made in the dataset\n",
    "        \n",
    "        Returns:\n",
    "            finetuned_model: dict: model fine tuned on the data and the tokenizer\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, model_name, model, tokenizer, tokenized_dataset, k_split=1, max_length=250, train_type=\"fine_tune\"):\n",
    "        \"\"\"\n",
    "        An interface function to train the model as per the training type\n",
    "\n",
    "        Args:\n",
    "            train_type: str: type of the training for the model\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        train_map = {\n",
    "            \"icl\": self._icl_model,\n",
    "            \"fine_tune\": self._fine_tune_model\n",
    "        }\n",
    "\n",
    "        if train_type not in train_map:\n",
    "            raise ValueError(\"Train type not found\")\n",
    "        \n",
    "        return train_map[train_type](\n",
    "            model_name, model, tokenizer, tokenized_dataset, k_split=1, max_length=250\n",
    "        )\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        A function that implements the evaluation strategy for model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def inference(self, data, model, tokenizer, max_input_tokens=1000, max_output_tokens=250):\n",
    "        \"\"\"\n",
    "        A function to perform model inferencing\n",
    "\n",
    "        Args:\n",
    "            data: str: data sample to infer on\n",
    "            model: object: fine tuned model to perform the model inferencing on\n",
    "            tokenizer: object: tokenizer to encode the sample text\n",
    "            max_input_tokens: int: number of input tokens to consider\n",
    "            max_output_tokens: int: number of the output tokens to consider\n",
    "        \n",
    "        Returns:\n",
    "            generated_text_answer: str: text generated from the model\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        input_ids = tokenizer.encode(\n",
    "                data,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_input_tokens\n",
    "        )\n",
    "\n",
    "        # Generate\n",
    "        device = model.device\n",
    "        generated_tokens_with_prompt = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            max_length=max_output_tokens\n",
    "        )\n",
    "\n",
    "        # Decode\n",
    "        generated_text_with_prompt = tokenizer.batch_decode(\n",
    "            generated_tokens_with_prompt, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Strip the prompt\n",
    "        generated_text_answer = generated_text_with_prompt[0][len(data):]\n",
    "\n",
    "        return generated_text_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, model_name: str, dataset_path: str):\n",
    "        self.preprocess_handler = Preprocess()\n",
    "        self.model_downloader = DownloadModel()\n",
    "        self.postprocess_handler = Postprocess()\n",
    "        self.model_mechanix = Model()\n",
    "        self.model_name = model_name\n",
    "        self.data_path = dataset_path\n",
    "\n",
    "        # download the model\n",
    "        self.model = self.model_downloader.download(self.model_name)\n",
    "    \n",
    "    def run(self, template_type=\"fine_tune\"):\n",
    "        \"\"\"\n",
    "        A function to train the model \n",
    "        \n",
    "        Args:\n",
    "            template_type: str: type of the template to consider\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # preprocess the data\n",
    "        self.data = self.preprocess_handler.preprocess(\n",
    "            # template_type, \n",
    "            # model_name=self.model_name,\n",
    "            tokenizer=self.model[\"tokenizer\"],\n",
    "            data_path=self.data_path\n",
    "        )\n",
    "        \n",
    "        # train the model\n",
    "        self.ft_model = self.model_mechanix.train(\n",
    "            model_name=self.model_name, \n",
    "            model=self.model[\"model\"], \n",
    "            tokenizer=self.model[\"tokenizer\"], \n",
    "            tokenized_dataset=self.data, \n",
    "            k_split=1, \n",
    "            max_length=250, \n",
    "            train_type=template_type\n",
    "        )\n",
    "\n",
    "        # evaluate the model\n",
    "        # self.eval_data = self.model_mechanix.evaluate(\n",
    "        #     model_name=model_name,\n",
    "        #     model=self.model[\"model\"],\n",
    "        #     tokenizer=self.model[\"tokenizer\"],\n",
    "        #     data=self.data\n",
    "        # )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
